{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "CNN V2 (Google colab version).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR2AKUwGoBbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5806c67e-ab68-48f1-b620-55556e1736d3"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "configuration = tf.compat.v1.ConfigProto()\n",
        "configuration.gpu_options.allow_growth = True\n",
        "session = tf.compat.v1.Session(config=configuration)\n",
        "\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "from tensorflow.keras.optimizers import SGD, Adam, schedules\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import keras\n",
        "\n",
        "import numpy as np\n",
        "import h5py\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "!pip install --upgrade tables"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tables\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/cb/4097be890a773af95343389faa8c283b0d9ff606f144227a548461dcbdd5/tables-3.6.1-cp37-cp37m-manylinux1_x86_64.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numexpr>=2.6.2 in /usr/local/lib/python3.7/dist-packages (from tables) (2.7.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.3 in /usr/local/lib/python3.7/dist-packages (from tables) (1.19.5)\n",
            "Installing collected packages: tables\n",
            "  Found existing installation: tables 3.4.4\n",
            "    Uninstalling tables-3.4.4:\n",
            "      Successfully uninstalled tables-3.4.4\n",
            "Successfully installed tables-3.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNkYRpVqBfnw"
      },
      "source": [
        "from numpy.random import seed\n",
        "seed(1)\n",
        "tf.random.set_seed(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlRr1LtopRyP",
        "outputId": "e90a9768-e71d-4df0-869f-1b665b48dc12"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_FstcQatLUK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bd2f13e-6885-417c-b61b-cbb41f4bf46d"
      },
      "source": [
        "folder = \"/content/drive/My Drive/\"\n",
        "\n",
        "reread = pd.read_hdf(folder+\"labeled_data_pickle4.hdf5\", key='FR')\n",
        "countries = ['AT', 'BE', 'BG', 'CY', 'CZ', 'DE', 'DK', 'EE', 'EL', 'ES', 'HR', 'HU', 'IE', 'LT', 'LU', 'LV', 'MT', 'NL', 'PL', 'PT', 'RO', 'SE', 'SI', 'SK', 'UK', 'IT'] #ES FR\n",
        "for country in countries:\n",
        "    temppd = pd.read_hdf(folder+\"labeled_data_pickle4.hdf5\", key=country)\n",
        "    reread = pd.concat((reread, temppd), ignore_index = True)\n",
        "temppd = None\n",
        "print('done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBxY8AZW6dcj",
        "outputId": "a4f18b5f-a2f5-409a-9042-2a5f7af84f5c"
      },
      "source": [
        "folder = \"/content/drive/My Drive/\"\n",
        "tempdf = pd.read_csv(folder+\"LUCAS_Topsoil_2015_20200323.csv\" ,usecols=[\"Point_ID\", \"LC1_Desc\"])\n",
        "reread = pd.merge(reread, tempdf, on='Point_ID', how='left')\n",
        "\n",
        "all_landscapes = reread[\"LC1_Desc\"].unique()\n",
        "\n",
        "print(len(reread))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20559\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXwt-TjRBXfW",
        "outputId": "b9d9294f-cf60-4416-b66c-b26164132625"
      },
      "source": [
        "filtered_df = pd.DataFrame(columns=reread.columns)\n",
        "\n",
        "for landscape in all_landscapes:\n",
        "  temppd  = reread.loc[reread['LC1_Desc'] == landscape] #common wheat\n",
        "  Q1 = temppd['OC'].quantile(0.25)\n",
        "  Q3 = temppd['OC'].quantile(0.75)\n",
        "  IQR = Q3 - Q1\n",
        "  temppd = temppd[temppd['OC'] < Q3 + IQR * 1.5]\n",
        "  temppd = temppd[temppd['OC'] > Q1 - IQR * 1.5]\n",
        "  filtered_df = filtered_df.append(temppd)\n",
        "\n",
        "reread = filtered_df\n",
        "filtered_df = None\n",
        "print(len(reread))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s3UWq5ZsXT2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjozM41N146z"
      },
      "source": [
        "X = np.array(list(reread['spectogram'].values))\n",
        "y = reread['OC_state'].values\n",
        "reread = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN0RCOuRPZp3"
      },
      "source": [
        "lb = LabelBinarizer()\n",
        "y = lb.fit_transform(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpXqQDG_PaNE"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "X = None\n",
        "y = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or-XvXMfPfF5"
      },
      "source": [
        "reg = l2(0.0005)\n",
        "init=\"he_normal\"\n",
        "chanDim = -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXVlpsdkPm62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "19bd7f61-e210-45e1-8556-c73e91904edc"
      },
      "source": [
        "opts = [\"Ones\", \"GlorotNormal\", \n",
        "        \"GlorotUniform\", \"Orthogonal\", \"Constant\", \"VarianceScaling\", \"lecun_uniform\", \"he_uniform\", \"LecunNormal\"]  #TruncatedNormal\n",
        "\n",
        "for optie in opts:\n",
        "  init = optie\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (7, 7), strides=(2, 2), padding=\"valid\",\n",
        "              kernel_initializer=init, kernel_regularizer=reg,\n",
        "              input_shape=(217,335,3)))\n",
        "  # here we stack two CONV layers on top of each other where\n",
        "  # each layerswill learn a total of 32 (3x3) filters\n",
        "  model.add(Conv2D(32, (3, 3), padding=\"same\",\n",
        "      kernel_initializer=init, kernel_regularizer=reg))\n",
        "  model.add(Activation(\"relu\"))\n",
        "  model.add(BatchNormalization(axis=chanDim))\n",
        "  model.add(Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\",\n",
        "      kernel_initializer=init, kernel_regularizer=reg))\n",
        "  model.add(Activation(\"relu\"))\n",
        "  model.add(BatchNormalization(axis=chanDim))\n",
        "  model.add(Dropout(0.25))\n",
        "  # stack two more CONV layers, keeping the size of each filter\n",
        "  # as 3x3 but increasing to 64 total learned filters\n",
        "  model.add(Conv2D(64, (3, 3), padding=\"same\",\n",
        "      kernel_initializer=init, kernel_regularizer=reg))\n",
        "  model.add(Activation(\"relu\"))\n",
        "  model.add(BatchNormalization(axis=chanDim))\n",
        "  model.add(Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\",\n",
        "      kernel_initializer=init, kernel_regularizer=reg))\n",
        "  model.add(Activation(\"relu\"))\n",
        "  model.add(BatchNormalization(axis=chanDim))\n",
        "  model.add(Dropout(0.25))\n",
        "  # increase the number of filters again, this time to 128\n",
        "  model.add(Conv2D(128, (3, 3), padding=\"same\",\n",
        "      kernel_initializer=init, kernel_regularizer=reg))\n",
        "  model.add(Activation(\"relu\"))\n",
        "  model.add(BatchNormalization(axis=chanDim))\n",
        "\n",
        "\n",
        "  # fully-connected layer\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512, kernel_initializer=init))\n",
        "  model.add(Activation(\"selu\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "  # softmax classifier\n",
        "  model.add(Dense((len(y_train[0]))))\n",
        "  model.add(Activation(\"softmax\"))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', metrics='accuracy', optimizer=\"adamax\")\n",
        "  history = model.fit(X_train, y_train,\n",
        "                  batch_size=64,\n",
        "                  epochs=8,\n",
        "                  verbose=1, shuffle=True)\n",
        "\n",
        "  score = model.evaluate(X_test, y_test)\n",
        "  print(optie)\n",
        "  print('Test score:', score[0])\n",
        "  print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "101/224 [============>.................] - ETA: 14:41 - loss: 75.9225 - accuracy: 0.1859"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-e30d35eb8a21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                   verbose=1, shuffle=True)\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdN1PpuWRvVA",
        "outputId": "3ec91bd9-4300-400a-8a68-8660b94cafb6"
      },
      "source": [
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-2,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "opt = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', metrics='accuracy', optimizer=\"adamax\")\n",
        "history = model.fit(X_train, y_train,\n",
        "                 batch_size=64,\n",
        "                 epochs=8,\n",
        "                 verbose=1, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "224/224 [==============================] - 41s 178ms/step - loss: 3.2380 - accuracy: 0.3818\n",
            "Epoch 2/8\n",
            "224/224 [==============================] - 40s 180ms/step - loss: 1.6248 - accuracy: 0.5870\n",
            "Epoch 3/8\n",
            "224/224 [==============================] - 40s 179ms/step - loss: 1.3761 - accuracy: 0.6174\n",
            "Epoch 4/8\n",
            "224/224 [==============================] - 41s 181ms/step - loss: 1.3118 - accuracy: 0.6286\n",
            "Epoch 5/8\n",
            "224/224 [==============================] - 40s 180ms/step - loss: 1.2530 - accuracy: 0.6264\n",
            "Epoch 6/8\n",
            "224/224 [==============================] - 40s 180ms/step - loss: 1.2233 - accuracy: 0.6440\n",
            "Epoch 7/8\n",
            "224/224 [==============================] - 40s 180ms/step - loss: 1.1809 - accuracy: 0.6478\n",
            "Epoch 8/8\n",
            "224/224 [==============================] - 40s 180ms/step - loss: 1.1364 - accuracy: 0.6662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzKzVOkhTMgH",
        "outputId": "6754c1e5-b274-4ab5-adf5-703163a7bbc0"
      },
      "source": [
        "score = model.evaluate(X_test, y_test)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "150/150 [==============================] - 4s 28ms/step - loss: 1.2281 - accuracy: 0.6599\n",
            "Test score: 1.228102207183838\n",
            "Test accuracy: 0.6598953008651733\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWHXkZj3XOTR"
      },
      "source": [
        "#Test score: 0.9720392227172852\n",
        "#Test accuracy: 0.6349738240242004\n",
        "\n",
        "#Test score: 1.0995421409606934\n",
        "#Test accuracy: 0.6544502377510071\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94gHXMuTxipz",
        "outputId": "ec6de788-cdb4-4dd3-daa8-81cb98c4f2ce"
      },
      "source": [
        "opts = [\"random_normal\", \"RandomUniform\", \"TruncatedNormal\", \"Zeros\", \"Ones\", \"GlorotNormal\", \n",
        "        \"GlorotUniform\", \"Orthogonal\", \"Constant\", \"VarianceScaling\", \"lecun_uniform\", \"he_uniform\", \"LecunNormal\"]\n",
        "for op in opts:\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (7, 7), strides=(2, 2), padding=\"valid\",\n",
        "                kernel_initializer=op, kernel_regularizer=reg,\n",
        "                input_shape=(217,335,3)))\n",
        "  print(op)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "random_normal\n",
            "RandomUniform\n",
            "TruncatedNormal\n",
            "Zeros\n",
            "Ones\n",
            "GlorotNormal\n",
            "GlorotUniform\n",
            "Orthogonal\n",
            "Constant\n",
            "VarianceScaling\n",
            "lecun_uniform\n",
            "he_uniform\n",
            "LecunNormal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqPTUonhxp0m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}