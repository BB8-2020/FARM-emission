{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CNN V2 (Google colab version).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR2AKUwGoBbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1db03c46-434c-4ac5-e730-0844195758ac"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "configuration = tf.compat.v1.ConfigProto()\n",
        "configuration.gpu_options.allow_growth = True\n",
        "session = tf.compat.v1.Session(config=configuration)\n",
        "\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "from tensorflow.keras.optimizers import SGD, Adam, schedules\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import keras\n",
        "\n",
        "import numpy as np\n",
        "import h5py\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "!pip install --upgrade tables"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tables\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/cb/4097be890a773af95343389faa8c283b0d9ff606f144227a548461dcbdd5/tables-3.6.1-cp37-cp37m-manylinux1_x86_64.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numexpr>=2.6.2 in /usr/local/lib/python3.7/dist-packages (from tables) (2.7.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.3 in /usr/local/lib/python3.7/dist-packages (from tables) (1.19.5)\n",
            "Installing collected packages: tables\n",
            "  Found existing installation: tables 3.4.4\n",
            "    Uninstalling tables-3.4.4:\n",
            "      Successfully uninstalled tables-3.4.4\n",
            "Successfully installed tables-3.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlRr1LtopRyP",
        "outputId": "3184d767-c545-4334-d885-98588735cbe7"
      },
      "source": [
        "\"\"\"This section is used to reach the files in my google drive, when using this code offline or localy, remove this cell of code.\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZODM0vm4w4cK"
      },
      "source": [
        "def data_setup():\n",
        "  \"\"\"This function sets up all the data needed to complile maken and train the CNN model\"\"\"\n",
        "\n",
        "  # Load data\n",
        "  \"\"\"This part is used to read the data off of the dataset file (hdf5), and turn it into a pandas dataframe\n",
        "  The data hdf5 file was created with pickle 4 protocol to support python 3.7\"\"\"\n",
        "\n",
        "  print('loading data...')\n",
        "  folder = \"/content/drive/My Drive/\" # change to folder that contains the hdf5 data file\n",
        "\n",
        "  reread = pd.read_hdf(folder+\"labeled_data_pickle4.hdf5\", key='FR')\n",
        "  countries = ['AT', 'BE', 'BG', 'CY', 'CZ', 'DE', 'DK', 'EE', 'EL', 'ES', 'HR', 'HU', 'IE', 'LT', 'LU', 'LV', 'MT', 'NL', 'PL', 'PT', 'RO', 'SE', 'SI', 'SK', 'UK', 'IT'] #ES FR\n",
        "  for country in countries:\n",
        "      temppd = pd.read_hdf(folder+\"labeled_data_pickle4.hdf5\", key=country)\n",
        "      reread = pd.concat((reread, temppd), ignore_index = True)\n",
        "  temppd = None\n",
        "  print('loading data: done\\n')\n",
        "\n",
        "  \n",
        "  # Add landscape column\n",
        "  \"\"\"This part adds the column 'LC1_Desc' which tells which type of landscape the data point is\"\"\"\n",
        "\n",
        "  print(\"Adding landscape column...\")\n",
        "  folder = \"/content/drive/My Drive/\" # change to folder that contains the csv data file, containing the landscape descriptions and point id's\n",
        "  tempdf = pd.read_csv(folder+\"LUCAS_Topsoil_2015_20200323.csv\" ,usecols=[\"Point_ID\", \"LC1_Desc\"])\n",
        "  reread = pd.merge(reread, tempdf, on='Point_ID', how='left')\n",
        "\n",
        "  all_landscapes = reread[\"LC1_Desc\"].unique()\n",
        "  print(\"Adding lanscape column: Done\\n\")\n",
        "\n",
        "\n",
        "  # Remove outliers\n",
        "  \"\"\"This part filters all datapoints their outliers for each landscape type.\n",
        "  This is measured based on the outliers of the OC value column\"\"\"\n",
        "\n",
        "  print(\"Removing outliers...\")\n",
        "  filtered_df = pd.DataFrame(columns=reread.columns)\n",
        "\n",
        "  for landscape in all_landscapes:\n",
        "    temppd  = reread.loc[reread['LC1_Desc'] == landscape] #common wheat\n",
        "    Q1 = temppd['OC'].quantile(0.25)\n",
        "    Q3 = temppd['OC'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    temppd = temppd[temppd['OC'] < Q3 + IQR * 1.5]\n",
        "    temppd = temppd[temppd['OC'] > Q1 - IQR * 1.5]\n",
        "    filtered_df = filtered_df.append(temppd)\n",
        "\n",
        "  reread = filtered_df\n",
        "  filtered_df = None\n",
        "  print(\"Removing outliers: Done\\n\")\n",
        "\n",
        "  # Setup data\n",
        "  \"\"\"This part sets up the data to use it for the CNN model, First it splits\n",
        "  the data from the input values and labels (x and y), then binarizes the labels.\n",
        "  After this the all the data is plit into train and test data\"\"\"\n",
        "\n",
        "  print(\"Setup data...\")\n",
        "  X = np.array(list(reread['spectogram'].values))\n",
        "  y = reread['OC_state'].values\n",
        "  reread = None\n",
        "  lb = LabelBinarizer()\n",
        "  y = lb.fit_transform(y)\n",
        "\n",
        "  label_lenght = len(y[0])\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "  X = None\n",
        "  y = None\n",
        "  print(\"Setup data: Done\\n\")\n",
        "  return label_lenght, X_train, X_test, y_train, y_test"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXVlpsdkPm62"
      },
      "source": [
        "def setup_model(label_train_length):\n",
        "  \"\"\"This function sets up the whole model for the CNN\"\"\"\n",
        "  print(\"Making model...\")\n",
        "\n",
        "  #Setup key parameters\n",
        "  reg = l2(0.0005)\n",
        "  init=\"he_normal\"\n",
        "  chanDim = -1\n",
        "\n",
        "  # The model:\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (7, 7), strides=(2, 2), padding=\"valid\",\n",
        "              kernel_initializer=init, kernel_regularizer=reg,\n",
        "              input_shape=(217,335,3)))\n",
        "  # here we stack two CONV layers on top of each other where\n",
        "  # each layerswill learn a total of 32 (3x3) filters\n",
        "  model.add(Conv2D(32, (3, 3), padding=\"same\",\n",
        "      kernel_initializer=init, kernel_regularizer=reg))\n",
        "  model.add(Activation(\"relu\"))\n",
        "  model.add(BatchNormalization(axis=chanDim))\n",
        "  model.add(Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\",\n",
        "      kernel_initializer=init, kernel_regularizer=reg))\n",
        "  model.add(Activation(\"relu\"))\n",
        "  model.add(BatchNormalization(axis=chanDim))\n",
        "  model.add(Dropout(0.25))\n",
        "  # stack two more CONV layers, keeping the size of each filter\n",
        "  # as 3x3 but increasing to 64 total learned filters\n",
        "  model.add(Conv2D(64, (3, 3), padding=\"same\",\n",
        "      kernel_initializer=init, kernel_regularizer=reg))\n",
        "  model.add(Activation(\"relu\"))\n",
        "  model.add(BatchNormalization(axis=chanDim))\n",
        "  model.add(Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\",\n",
        "      kernel_initializer=init, kernel_regularizer=reg))\n",
        "  model.add(Activation(\"relu\"))\n",
        "  model.add(BatchNormalization(axis=chanDim))\n",
        "  model.add(Dropout(0.25))\n",
        "  # increase the number of filters again, this time to 128\n",
        "  model.add(Conv2D(128, (3, 3), padding=\"same\",\n",
        "      kernel_initializer=init, kernel_regularizer=reg))\n",
        "  model.add(Activation(\"relu\"))\n",
        "  model.add(BatchNormalization(axis=chanDim))\n",
        "\n",
        "\n",
        "  # fully-connected layer\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512, kernel_initializer=init))\n",
        "  model.add(Activation(\"selu\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "  # softmax classifier\n",
        "  model.add(Dense(label_train_length))\n",
        "  model.add(Activation(\"softmax\"))\n",
        "  print(\"Making model: Done\\n\")\n",
        "  return model\n",
        "\n",
        "def train_model(model, X_train, y_train):\n",
        "  \"\"\"This function trains the model\"\"\"\n",
        "  print(\"Training model...\")\n",
        "  model.compile(loss='categorical_crossentropy', metrics='accuracy', optimizer=\"adamax\")\n",
        "  history = model.fit(X_train, y_train,\n",
        "                  batch_size=64,\n",
        "                  epochs=8,\n",
        "                  verbose=1, shuffle=True)\n",
        "  return model\n",
        "  print(\"Training model: Done\\n\")\n",
        "\n",
        "def score_model(model, X_test, y_test):\n",
        "  \"\"\"This function scores the models perfomance and prints it out\"\"\"\n",
        "  score = model.evaluate(X_test, y_test)\n",
        "\n",
        "  print('Test score:', score[0])\n",
        "  print('Test accuracy:', score[1])\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBT4xuWUzr7-",
        "outputId": "3fa5e9a7-d7dc-4fec-c63a-ffd8e7e6e512"
      },
      "source": [
        "# setup data, only needed if not already done\n",
        "label_length, X_train, X_test, y_train, y_test = data_setup()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data...\n",
            "loading data: done\n",
            "\n",
            "Adding landscape column...\n",
            "Adding lanscape column: Done\n",
            "\n",
            "Removing outliers...\n",
            "Removing outliers: Done\n",
            "\n",
            "Setup data...\n",
            "Setup data: Done\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdN1PpuWRvVA",
        "outputId": "ae9c3ee0-7e7f-4c83-de8d-5bf0395f8591"
      },
      "source": [
        "\n",
        "# Train and score model\n",
        "model = setup_model(lenght_label)\n",
        "model = train_model(model, X_train, y_train)\n",
        "score_model(model, X_test, y_test)\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Making model...\n",
            "Making model: Done\n",
            "\n",
            "Training model...\n",
            "Epoch 1/8\n",
            "224/224 [==============================] - 30s 127ms/step - loss: 3.3245 - accuracy: 0.3554\n",
            "Epoch 2/8\n",
            "224/224 [==============================] - 29s 128ms/step - loss: 1.7057 - accuracy: 0.5665\n",
            "Epoch 3/8\n",
            "224/224 [==============================] - 29s 129ms/step - loss: 1.4231 - accuracy: 0.6044\n",
            "Epoch 4/8\n",
            "224/224 [==============================] - 30s 133ms/step - loss: 1.3360 - accuracy: 0.6140\n",
            "Epoch 5/8\n",
            "224/224 [==============================] - 30s 134ms/step - loss: 1.3047 - accuracy: 0.6195\n",
            "Epoch 6/8\n",
            "224/224 [==============================] - 30s 132ms/step - loss: 1.2461 - accuracy: 0.6317\n",
            "Epoch 7/8\n",
            "224/224 [==============================] - 30s 134ms/step - loss: 1.1995 - accuracy: 0.6428\n",
            "Epoch 8/8\n",
            "224/224 [==============================] - 30s 134ms/step - loss: 1.1904 - accuracy: 0.6315\n",
            "150/150 [==============================] - 3s 20ms/step - loss: 1.1450 - accuracy: 0.6561\n",
            "Test score: 1.1450220346450806\n",
            "Test accuracy: 0.6561256647109985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWHXkZj3XOTR"
      },
      "source": [
        "#Test score: 0.9720392227172852\n",
        "#Test accuracy: 0.6349738240242004\n",
        "\n",
        "#Test score: 1.0995421409606934\n",
        "#Test accuracy: 0.6544502377510071]\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBZzpsSA7f6P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}